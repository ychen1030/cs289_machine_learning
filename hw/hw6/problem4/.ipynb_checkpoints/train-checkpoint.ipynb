{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EIfG6ySSanCl"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils import data\n",
    "from mds189 import Mds189\n",
    "import numpy as np\n",
    "from skimage import io, transform\n",
    "import ipdb\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "import torchvision.models as models\n",
    "from PIL import Image\n",
    "import time\n",
    "start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0NHrTXRhfP0t"
   },
   "outputs": [],
   "source": [
    "# Helper functions for loading images.\n",
    "def pil_loader(path):\n",
    "    # open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\n",
    "    with open(path, 'rb') as f:\n",
    "        img = Image.open(f)\n",
    "        return img.convert('RGB')\n",
    "\n",
    "def accimage_loader(path):\n",
    "    import accimage\n",
    "    try:\n",
    "        return accimage.Image(path)\n",
    "    except IOError:\n",
    "        # Potentially a decoding problem, fall back to PIL.Image\n",
    "        return pil_loader(path)\n",
    "\n",
    "def default_loader(path):\n",
    "    from torchvision import get_image_backend\n",
    "    if get_image_backend() == 'accimage':\n",
    "        return accimage_loader(path)\n",
    "    else:\n",
    "        return pil_loader(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W7QnfQ3AfqJ0"
   },
   "outputs": [],
   "source": [
    "# flag for whether you're training or not\n",
    "is_train = True\n",
    "is_key_frame = True # TODO: set this to false to train on the video frames, instead of the key frames\n",
    "model_to_load = 'model.ckpt' # This is the model to load during testing, if you want to eval a previously-trained model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PBHv12eEfttV"
   },
   "outputs": [],
   "source": [
    "# CUDA for PyTorch\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "#cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JoX-V6Fgf1Ip"
   },
   "outputs": [],
   "source": [
    "# Parameters for data loader\n",
    "params = {'batch_size': 25,  # TODO: fill in the batch size. often, these are things like 32,64,128,or 256\n",
    "          'shuffle': True,\n",
    "          'num_workers': 2 \n",
    "          }\n",
    "# TODO: Hyper-parameters\n",
    "num_epochs = 25\n",
    "learning_rate = 1e-3\n",
    "# NOTE: depending on your optimizer, you may want to tune other hyperparameters as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JivHzthZf7HR"
   },
   "outputs": [],
   "source": [
    "# Datasets\n",
    "# TODO: put the path to your train, test, validation txt files\n",
    "if is_key_frame:\n",
    "    label_file_train =  './dataloader_files/keyframe_data_train.txt'\n",
    "    label_file_val  =  './dataloader_files/keyframe_data_val.txt'\n",
    "    # NOTE: the kaggle competition test data is only for the video frames, not the key frames\n",
    "    # this is why we don't have an equivalent label_file_test with keyframes\n",
    "else:\n",
    "    label_file_train = './dataloader_files/videoframe_data_train.txt'\n",
    "    label_file_val = './dataloader_files/videoframe_data_val.txt'\n",
    "    label_file_test = './dataloader_files/videoframe_data_test.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nYxdIg53gLvB"
   },
   "outputs": [],
   "source": [
    "# TODO: you should normalize based on the average image in the training set. This shows \n",
    "# an example of doing normalization\n",
    "mean = [0.5, 0.5, 0.5]\n",
    "std = [0.5, 0.5, 0.5]\n",
    "# TODO: if you want to pad or resize your images, you can put the parameters for that below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-6eKsAwcgUex"
   },
   "outputs": [],
   "source": [
    "# Generators\n",
    "# NOTE: if you don't want to pad or resize your images, you should delete the Pad and Resize\n",
    "# transforms from all three _dataset definitions.\n",
    "train_dataset = Mds189(label_file_train,loader=default_loader,transform=transforms.Compose([\n",
    "                                               transforms.Pad(padding=0),    # TODO: if you want to pad your images\n",
    "                                               transforms.Resize((32, 32)), # TODO: if you want to resize your images\n",
    "                                               transforms.ToTensor(),\n",
    "                                               transforms.Normalize(mean, std)\n",
    "                                           ]))\n",
    "train_loader = data.DataLoader(train_dataset, **params)\n",
    "\n",
    "val_dataset = Mds189(label_file_val,loader=default_loader,transform=transforms.Compose([\n",
    "                                               transforms.Pad(padding=0),\n",
    "                                               transforms.Resize((32, 32)),\n",
    "                                               transforms.ToTensor(),\n",
    "                                               transforms.Normalize(mean, std)\n",
    "                                           ]))\n",
    "val_loader = data.DataLoader(val_dataset, **params)\n",
    "\n",
    "if not is_key_frame:\n",
    "    test_dataset = Mds189(label_file_test,loader=default_loader,transform=transforms.Compose([\n",
    "                                                   transforms.Pad(padding=0),\n",
    "                                                   transforms.Resize((32, 32)),\n",
    "                                                   transforms.ToTensor(),\n",
    "                                                   transforms.Normalize(mean, std)\n",
    "                                               ]))\n",
    "    test_loader = data.DataLoader(test_dataset, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gK09h_RngY_b"
   },
   "outputs": [],
   "source": [
    "# TODO: one way of defining your model architecture is to fill in a class like NeuralNet()\n",
    "# NOTE: you should not overwrite the models you try whose performance you're keeping track of.\n",
    "#       one thing you could do is have many different model forward passes in class NeuralNet()\n",
    "#       and then depending on which model you want to train/evaluate, you call that model's\n",
    "#       forward pass. this strategy will save you a lot of time in the long run. the last thing\n",
    "#       you want to do is have to recode the layer structure for a model (whose performance\n",
    "#       you're reporting) because you forgot to e.g., compute the confusion matrix on its results\n",
    "#       or visualize the error modes of your (best) model\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        # you can define some common layers, for example: \n",
    "        self.conv1 = nn.Conv2d(3, 6, 5) # you should review the definition of nn.Conv2d online\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        # note: input_dimensions and output_dimensions are not defined, they\n",
    "        # are placeholders to show you what arguments to pass to nn.Linear \n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 8)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # now you can use the layers you defined, to write the forward pass, i.e.,\n",
    "        # network architecture for your model\n",
    "        x = self.pool(F.relu(self.conv1(x))) # x -> convolution -> ReLU -> max pooling\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        # Tensors need to be reshaped before going into an fc layer\n",
    "        # the -1 will correspond to the batch size\n",
    "        x = x.view(-1, self.num_flat_features(x)) \n",
    "        # x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x)) # x -> fc (affine) layer -> relu\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        print(size)\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(6 * 14 * 14, 120)\n",
    "        self.fc2 = nn.Linear(120, 8)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x))) # x -> convolution -> ReLU -> max pooling\n",
    "        x = x.view(-1, 6 * 14 * 14)\n",
    "        x = F.relu(self.fc1(x)) # x -> fc (affine) layer -> relu\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNet(\n",
      "  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=1176, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=8, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = NeuralNet()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_accuracy(model)\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        predicted_list = []\n",
    "        groundtruth_list = []\n",
    "        for (local_batch,local_labels) in val_loader:\n",
    "            # Transfer to GPU\n",
    "            local_ims, local_labels = local_batch.to(device), local_labels.to(device)\n",
    "\n",
    "            outputs = model.forward(local_ims)\n",
    "            loss = criterion(outputs, local_labels)\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KfoTJSoHgfvc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning training..\n",
      "Epoch [1/25], Step [50/117], Loss: 1.8888\n",
      "Epoch [1/25], Step [100/117], Loss: 1.5201\n",
      "Epoch [2/25], Step [50/117], Loss: 1.8271\n",
      "Epoch [2/25], Step [100/117], Loss: 1.3540\n",
      "Epoch [3/25], Step [50/117], Loss: 1.4472\n",
      "Epoch [3/25], Step [100/117], Loss: 1.5760\n",
      "Epoch [4/25], Step [50/117], Loss: 1.0656\n",
      "Epoch [4/25], Step [100/117], Loss: 1.0632\n",
      "Epoch [5/25], Step [50/117], Loss: 0.8550\n",
      "Epoch [5/25], Step [100/117], Loss: 0.7777\n",
      "Epoch [6/25], Step [50/117], Loss: 0.7451\n",
      "Epoch [6/25], Step [100/117], Loss: 0.9496\n",
      "Epoch [7/25], Step [50/117], Loss: 1.4170\n",
      "Epoch [7/25], Step [100/117], Loss: 0.5023\n",
      "Epoch [8/25], Step [50/117], Loss: 0.6468\n",
      "Epoch [8/25], Step [100/117], Loss: 0.3317\n",
      "Epoch [9/25], Step [50/117], Loss: 0.4815\n",
      "Epoch [9/25], Step [100/117], Loss: 0.5447\n",
      "Epoch [10/25], Step [50/117], Loss: 0.4575\n",
      "Epoch [10/25], Step [100/117], Loss: 0.4062\n",
      "Epoch [11/25], Step [50/117], Loss: 0.1173\n",
      "Epoch [11/25], Step [100/117], Loss: 0.4212\n",
      "Epoch [12/25], Step [50/117], Loss: 0.1002\n",
      "Epoch [12/25], Step [100/117], Loss: 0.1536\n",
      "Epoch [13/25], Step [50/117], Loss: 0.2068\n",
      "Epoch [13/25], Step [100/117], Loss: 0.2237\n",
      "Epoch [14/25], Step [50/117], Loss: 0.1511\n",
      "Epoch [14/25], Step [100/117], Loss: 0.1052\n",
      "Epoch [15/25], Step [50/117], Loss: 0.0862\n",
      "Epoch [15/25], Step [100/117], Loss: 0.0416\n",
      "Epoch [16/25], Step [50/117], Loss: 0.1004\n",
      "Epoch [16/25], Step [100/117], Loss: 0.0435\n",
      "Epoch [17/25], Step [50/117], Loss: 0.0785\n",
      "Epoch [17/25], Step [100/117], Loss: 0.1219\n",
      "Epoch [18/25], Step [50/117], Loss: 0.2285\n",
      "Epoch [18/25], Step [100/117], Loss: 0.0589\n",
      "Epoch [19/25], Step [50/117], Loss: 0.0183\n",
      "Epoch [19/25], Step [100/117], Loss: 0.0338\n",
      "Epoch [20/25], Step [50/117], Loss: 0.0180\n",
      "Epoch [20/25], Step [100/117], Loss: 0.0284\n",
      "Epoch [21/25], Step [50/117], Loss: 0.0369\n",
      "Epoch [21/25], Step [100/117], Loss: 0.0120\n",
      "Epoch [22/25], Step [50/117], Loss: 0.0533\n",
      "Epoch [22/25], Step [100/117], Loss: 0.0088\n",
      "Epoch [23/25], Step [50/117], Loss: 0.0326\n",
      "Epoch [23/25], Step [100/117], Loss: 0.0754\n",
      "Epoch [24/25], Step [50/117], Loss: 0.4064\n",
      "Epoch [24/25], Step [100/117], Loss: 0.0524\n",
      "Epoch [25/25], Step [50/117], Loss: 0.0894\n",
      "Epoch [25/25], Step [100/117], Loss: 0.1419\n",
      "Time: 42762.769622564316\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNet().to(device)\n",
    "\n",
    "# if we're only testing, we don't want to train for any epochs, and we want to load a model\n",
    "if not is_train:\n",
    "    num_epochs = 0\n",
    "    model.load_state_dict(torch.load('model.ckpt'))\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion =  nn.CrossEntropyLoss() #TODO: define your loss here. hint: should just require calling a built-in pytorch layer.\n",
    "# NOTE: you can use a different optimizer besides Adam, like RMSProp or SGD, if you'd like\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "# Loop over epochs\n",
    "print('Beginning training..')\n",
    "total_step = len(train_loader)\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "for epoch in range(num_epochs):\n",
    "    # Training\n",
    "    # print('epoch {}'.format(epoch))\n",
    "    for i, (local_batch,local_labels) in enumerate(train_loader):\n",
    "        # Transfer to GPU\n",
    "        local_ims, local_labels = local_batch.to(device), local_labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model.forward(local_ims)\n",
    "        loss = criterion(outputs, local_labels)\n",
    "        # TODO: maintain a list of your losses as a function of number of steps\n",
    "        #       because we ask you to plot this information\n",
    "        # NOTE: if you use Google Colab's tensorboard-like feature to visualize\n",
    "        #       the loss, you do not need to plot it here. just take a screenshot\n",
    "        #       of the loss curve and include it in your write-up.\n",
    "        # running_loss.append(loss.item())\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i+1) % 50 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
    "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
    "    \n",
    "    train_loss.append(loss.item())\n",
    "    val_loss.append()\n",
    "\n",
    "end = time.time()\n",
    "print('Time: {}'.format(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (5, 4) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "plt.plot(running_loss, 'o')\n",
    "plt.title('Training loss history')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Training loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VT88tdTDgozX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning Testing..\n",
      "Accuracy of the network on the 975 test images: 52.1025641025641 %\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "# In test phase, we don't need to compute gradients (for memory efficiency)\n",
    "print('Beginning Testing..')\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    predicted_list = []\n",
    "    groundtruth_list = []\n",
    "    for (local_batch,local_labels) in val_loader:\n",
    "        # Transfer to GPU\n",
    "        local_ims, local_labels = local_batch.to(device), local_labels.to(device)\n",
    "\n",
    "        outputs = model.forward(local_ims)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += local_labels.size(0)\n",
    "        predicted_list.extend(predicted)\n",
    "        groundtruth_list.extend(local_labels)\n",
    "        correct += (predicted == local_labels).sum().item()\n",
    "\n",
    "    print('Accuracy of the network on the {} test images: {} %'.format(total, 100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NcGFt7IMgsx-"
   },
   "outputs": [],
   "source": [
    "# Look at some things about the model results..\n",
    "# convert the predicted_list and groundtruth_list Tensors to lists\n",
    "pl = [p.cpu().numpy().tolist() for p in predicted_list]\n",
    "gt = [p.cpu().numpy().tolist() for p in groundtruth_list]\n",
    "\n",
    "# TODO: use pl and gt to produce your confusion matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8ZKxpA3PgyOv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reach: 0.52\n",
      "squat: 0.48\n",
      "inline: 0.6733333333333333\n",
      "lunge: 0.6133333333333333\n",
      "hamstrings: 0.49333333333333335\n",
      "stretch: 0.3933333333333333\n",
      "deadbug: 0.48\n",
      "pushup: 0.4666666666666667\n"
     ]
    }
   ],
   "source": [
    "# view the per-movement accuracy\n",
    "label_map = ['reach','squat','inline','lunge','hamstrings','stretch','deadbug','pushup']\n",
    "for id in range(len(label_map)):\n",
    "    print('{}: {}'.format(label_map[id],sum([p and g for (p,g) in zip(np.array(pl)==np.array(gt),np.array(gt)==id)])/(sum(np.array(gt)==id)+0.)))\n",
    "\n",
    "# TODO: you'll need to run the forward pass on the kaggle competition images, and save those results to a csv file.\n",
    "if not is_key_frame:\n",
    "    # your code goes here!\n",
    "    pass\n",
    "\n",
    "# Save the model checkpoint\n",
    "torch.save(model.state_dict(), 'model.ckpt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "train.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
